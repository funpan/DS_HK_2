{
 "metadata": {
  "name": "",
  "signature": "sha256:170a2f20e764b2799269dd7cef8d34dbabafb54b4eb296b485db425bbfb17749"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%javascript\n",
      "function is_local(){\n",
      "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
      "}\n",
      "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
      "$.getScript(url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "function is_local(){\n",
        "  return (document.location.hostname == \"localhost\" || document.location.hostname == '127.0.0.1')\n",
        "}\n",
        "var url = is_local() ? \"http://localhost:8000/theme/custom.js\" : \"http://odhk.github.io/hyrule_theme/custom.js\"\n",
        "$.getScript(url)"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Javascript at 0x234f850>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Polynomial Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Do we need to have 280 brands of breakfast cereal? No, probably not. But we have them for a reason - because some people like them. It's the same with baseball statistics.\n",
      "\n",
      "<footer>~ Bill James</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Polynomial Regression\n",
      "1. Assumptions Of Linear Regressions\n",
      "1. Signs Of Multicolinearity\n",
      "1. Regularization\n",
      "\n",
      "**Labs:**\n",
      "1. Ordinary Least Squares & Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Assumptions of Linear Regressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* linearity\n",
      "* reliability of measurement\n",
      "* homoscedasticity\n",
      "* normality"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "or more specifically"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Validity.\n",
      "1. Additivity and linearity.\n",
      "1. Independence of errors\n",
      "1. Equal variance of errors\n",
      "1. Normality of errors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "how to [break them](http://www.basic.northwestern.edu/statguidefiles/linreg_ass_viol.html), some [Transformations](http://www.basic.northwestern.edu/statguidefiles/linreg_alts.html#Transformations) to meet them, and [Common Mistakes on Intepreting Regressions](https://www.ma.utexas.edu/users/mks/statmistakes/regressioncoeffs.html)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Polynomial Regression\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following polynomial regression model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$y = \u03b1 + \u03b2_1x + \u03b2_2x^2 + \u03b5$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Q: This represents a nonlinear relationship. Is it still a linear model?**\n",
      "\n",
      "A: Yes, because it\u2019s linear in the \u03b2's!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> \u201cAlthough polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y|x) is linear in the unknown parameters that are estimated from the data. For this reason, polynomial regression is considered to be a special case of multiple linear regression.\u201d\n",
      "\n",
      "<footer>~ Wikipedia</footer>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Polynomial regression allows us to fit very complex\n",
      "curves to data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$y = \u03b1 + \u03b2_1x + \u03b2_2x^2 + ... + \u03b2_nx^n + \u03b5$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But there is one problem with the model we\u2019ve written\n",
      "down so far. This model violates one of the assumptions of linear regression!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This model displays multicollinearity, which means the\n",
      "predictor variables are highly correlated with each other."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "x = np.arange(1, 10, 0.1)\n",
      "np.corrcoef(x**9,x**10)[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "0.9987609495377574"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multicollinearity causes the linear regression model to break down, because it can\u2019t tell the predictor variables apart. This results in a singularity. We will see an example of this in just a minute!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: What can we do about this?**\n",
      "    \n",
      "A: Replace the correlated predictors with uncorrelated predictors.\n",
      "\n",
      "> <small>These polynomialfunctions form an orthogonal basis of the function space.</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$y = \u03b1 + \u03b2_1f_1(x) + \u03b2_2f_2(x^2) + ... + \u03b2_nf_n(x^n) + \u03b5$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far, we\u2019ve seen how polynomial regression allows us to fit complex nonlinear relationships, and even to avoid multicollinearity (by using basis functions)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q: Can a regression model be too complex?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Polynomials & MultiColinearity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A very simple [visual reminder](http://stats.stackexchange.com/questions/58739/polynomial-regression-using-scikit-learn) of what it means to add polynomials to your design matrix. Code with plot explaining why you'd want to [use polynomials](http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html) in the first place. Numpy also provides a [module](http://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) to directly deal with polynomials."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A simple [Standardization Technique](https://www.stat.fi/isi99/proceedings/arkisto/varasto/kim_0574.pdf) to deal with the MultiColinearity of Polynomials. This was implemented in the [excercise/health example](https://onlinecourses.science.psu.edu/stat501/node/85) we covered in class. Another way is to use [Orthoganal Polynomials](http://dlmf.nist.gov/18.4) to deal with multicolinearity. Finally, an academic paper discussing how to [Minimize the Effects of Colinearity](ftp://ftp.bgu.ac.il/shacham/publ_papers/IandEC_36_4405_97.pdf)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Signs of Multicolinearity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* A regression coefficient is not significant even though, theoretically, that variable should be highly correlated with Y.\n",
      "* When you add or delete an X variable, the regression coefficients change dramatically.\n",
      "* You see a negative regression coefficient when your response should increase along with X.\n",
      "* You see a positive regression coefficient when the response should decrease as X increases.\n",
      "* Your X variables have high pairwise correlations.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Formally, we can check the tolerance values or `variance inflation ratio` (VIF) to investigate possible collinearity. We define the `tolerance` as 1/ R2 and `VIF` as 1/Tolerance.\n",
      "\n",
      "The less the tolerance\u2019s value, (or it is closer to zero, or < 0.1), the worse of the collinearity. This is conforms to the formula: as tolerance close to zero then R2 is closer to 1, meaning a stronger linear relation.\n",
      "\n",
      "It is not surprising, since VIF is the reciprocal of the tolerance, then the larger of the value of VIF, the worse the collinearity!\n",
      "\n",
      "Usually, if VIF is greater than 10 we should consider it a warning sign! Under the situation when there is collinearity, we may reasonably consider using only one of the correlated variables (ignore the other one, it does not matter which one to choose staying in the model). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Regularization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: What\u2019s overfitting?**\n",
      "\n",
      "Overfitting occurs when a model matches the noise\n",
      "instead of the signal."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Noise**: Extra \u201ccruft\u201d that doesn\u2019t contribute to a readable\n",
      "prediction.\n",
      "**Signal**: Clean, elegant interpretation of the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This happens when our model is too complex!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Overfitting (Classification)\n",
      "![](assets/overfitting_classification.png) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Overfitting (Regression)\n",
      "![](assets/overfitting_regression.jpg)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: How do we define the complexity of a regression\n",
      "model?**\n",
      "\n",
      "A: One method is to define **complexity** as a function of the size of the coefficients.\n",
      "\n",
      "$Ex1: \\Sigma \\lVert \\beta_i \\rVert$ , this is called the **L1-norm**\n",
      "\n",
      "$Ex2: \\Sigma \\beta_i^2 $$ $ , this is called the **L2-norm**\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These measures of complexity lead to the following\n",
      "regularization techniques:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### L1 regularization:\n",
      "$$y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i \\rVert \\lt s$$\n",
      "    \n",
      "#### L2 regularization:\n",
      "$$y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \u03b2_i^2 \\lt s$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regularization refers to the method of preventing\n",
      "overfitting by explicitly controlling model complexity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These regularization problems can also be expressed as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### L1 regularization:\n",
      "$$ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert)$$\n",
      "    \n",
      "#### L2 regularization:\n",
      "$$ min(\\lVert y - x\u03b2 \\rVert^2 + \u03bb\\lVert x \\rVert^2)$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "but more importantly, we can think about the use cases of these two more clearly this way:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**L1 regularization**:  Used when we have small data but many features.\n",
      "\n",
      "**L2 regularization**: Used in just about all other cases."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This (Lagrangian) formulation reflects the fact that\n",
      "there is a cost associated with regularization."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clear introduction to the [L-1 and L-2 Norms](http://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/), the [effects of L1](http://cseweb.ucsd.edu/~saul/teaching/cse291s07/L1norm.pdf)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Bias and Variance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Bias** refers to predictions that are systematically\n",
      "inaccurate. \n",
      "* **Variance** refers to predictions that are generally\n",
      "inaccurate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/bias_variance_darts.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It turns out (after some math) that the generalization error in our model can be decomposed into a bias component and variance component."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### This is another example of the bias-variance tradeoff."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/bias_variance_tradeoff.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tradeoff is regulated by a hyperparameter $\u03bb$, which we\u2019ve already seen:\n",
      "\n",
      "#### L1 regularization\n",
      "\n",
      "$$y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i \\rVert \\lt \u03bb$$\n",
      "\n",
      "\n",
      "#### L2 regularization\n",
      "\n",
      "$$y=\u03a3\u03b2_ix_i + \u03b5 \\quad st. \\quad \u03a3 \\lVert \u03b2_i^2 \\rVert \\lt \u03bb$$\n",
      "\n",
      "We should take advantage of generalization to trade off variance in our data for bias in our fit, which will overall produce a clearer and better overall fit to our data!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Ordinary Least Squares & Ridge Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Practice using both the `LinearRegression` and `RidgeRegression` models in scikit-learn\n",
      "* Explore differences between a model using OLS (L1) and LLS (L2) regularization\n",
      "* Tear apart and understand how predictions get built in scikit learn\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Set some Pandas options\n",
      "pd.set_option('max_columns', 30)\n",
      "pd.set_option('max_rows', 20)\n",
      "\n",
      "# Store data in a consistent place\n",
      "DATA_DIR = '../data/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "More in depth understanding behind how scikit learn works"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Load up this script (using the same mammals set we glanced at before) and let's break down (and also break) what's going on with each line of code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "mammals = pd.read_csv(DATA_DIR + 'mammals.csv')\n",
      "\n",
      "lm = linear_model.LinearRegression()\n",
      "log_lm = linear_model.LinearRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "body = [ [x] for x in mammals['body'].values]\n",
      "brain = mammals['brain'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_body = [ [x] for x in np.log(mammals['body'].values)]\n",
      "log_brain = np.log(mammals['brain'].values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm.fit(body, brain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_lm.fit(log_body, log_brain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can observe various features about our linear models that we've discussed in lecture."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Find the intercept"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note we can set to train for an intercept with `set_params()`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm.intercept_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "91.004396207406927"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_lm.intercept_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "2.1347886767646358"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Print out the predictions for a given matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "needs to fit the same dimensional space as the data we fit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lm.predict(body)\n",
      "mammals['predict'] = lm.predict(body)\n",
      "mammals['predict']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "0     94.275986\n",
        "1     91.468314\n",
        "2     92.309166\n",
        "3    540.425207\n",
        "4    126.117209\n",
        "5    117.737686\n",
        "6    105.337537\n",
        "7     92.009552\n",
        "...\n",
        "54     91.050788\n",
        "55    276.571699\n",
        "56     93.903885\n",
        "57    245.643815\n",
        "58     91.874243\n",
        "59     92.570120\n",
        "60     91.104912\n",
        "61     95.097508\n",
        "Name: predict, Length: 62, dtype: float64"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_lm.predict(log_body)\n",
      "mammals['log_predict'] = np.exp(log_lm.predict(log_body))\n",
      "mammals['log_predict']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "0     21.144078\n",
        "1      4.869905\n",
        "2     10.594903\n",
        "3    855.487784\n",
        "4    125.879932\n",
        "5    102.552664\n",
        "6     64.188507\n",
        "7      8.708245\n",
        "...\n",
        "54      0.862650\n",
        "55    439.999684\n",
        "56     19.309556\n",
        "57    383.647997\n",
        "58      7.811450\n",
        "59     12.151123\n",
        "60      1.542569\n",
        "61     25.022105\n",
        "Name: log_predict, Length: 62, dtype: float64"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Practice: Plotting Predictions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since pyplot plots _discretely_ (and not continuously), if we want an accurate representation of any transformed plot (particularly for polynomial data), we need to sort our data frame by the response value. We can either do this before fitting a model (with the actual response) or post-fit (with the predicted response)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sort by response:\n",
      "mammals = mammals.sort('brain')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sort by prediction:\n",
      "mammals_log_sort = mammals.sort('log_predict')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Otherwise, use plt.scatter and plt.plot as we know how to use them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Polynominal regressions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Likewise, polynomial regressions can work as new inputs constructed from our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mammals['body_squared'] = mammals['body']**2\n",
      "body_squared = [ [x, y] for x,y in zip(mammals['body'].values, mammals['body_squared'].values)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# OR\n",
      "body_squared = [ [x, y] for x,y in zip(mammals['body'].values, (mammals['body'].values)**2)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The LinearRegression model in scikit learn uses L1-Normalization. Let's check out a Ridge Regression, which uses L2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ridge = linear_model.Ridge()\n",
      "ridge.fit(body_squared, brain)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
        "   normalize=False, solver='auto', tol=0.001)"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the coefficients and the intercept. and we can verify the results of ridge.predict() against handwriting the full regression:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "((ridge.coef_[1] * mammals['body'])**2) + ((ridge.coef_[0] * mammals['body'])) + ridge.intercept_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "13    312.246993\n",
        "39    312.245300\n",
        "19    312.240895\n",
        "54    312.232426\n",
        "38    312.240895\n",
        "14    312.228361\n",
        "37    312.208034\n",
        "52    312.223279\n",
        "...\n",
        "3      154.719045\n",
        "45     294.578220\n",
        "41     227.555218\n",
        "21     135.747794\n",
        "27     133.037617\n",
        "31     291.244683\n",
        "18    -550.596109\n",
        "32   -1941.863346\n",
        "Name: body, Length: 62, dtype: float64"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Work through the following datasets, determining best fits for each data set (predictor value/y value in parens). To better evaluate or improve this process, try including:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f_regression_feature_selection():\n",
      "from sklearn import feature_selection\n",
      "feature_selection.univariate_selection.f_regression(input, response)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'response' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-53-6d024d34bf69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munivariate_selection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and using this against your feature matrix to determine p-values for each feature (we care about the second array it returns for now).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Datasets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Predicting stopping distance](https://gist.github.com/tijptjik/03305bc2d75cda183439 )\n",
      "* [Predicting City and Highway MPG.](https://gist.github.com/tijptjik/3746ac7c1f0ec6953ed7 )"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://gist.githubusercontent.com/tijptjik/03305bc2d75cda183439/raw/5d0acdb9dc7a5b39d057c837ec650dd09c4e9b36/cars1920.csv >  ../data/cars1920.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "100   308    0   308    0     0    170      0 --:--:--  0:00:01 --:--:--   170\r\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!curl https://gist.githubusercontent.com/tijptjik/3746ac7c1f0ec6953ed7/raw/6561674a1c1edff3b7f78fcce326df5c72bf402e/cars93.csv >  ../data/cars93.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
        "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "100 14410    0 14410    0     0   6612      0 --:--:--  0:00:02 --:--:--  6613\r\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tips"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If your data isn't a good fit, what are some things you might try?\n",
      "\n",
      "1. Increase the number of training points N. This might give us a training set with more coverage, and lead to greater accuracy.\n",
      "1. Increase the degree d of the polynomial. This might allow us to more closely fit the training data, and lead to a better result\n",
      "1. Add more features. If we were to, for example, perform a linear regression using x, x\u221a, x\u22121, or other functions, we might hit on a functional form which can better be mapped to the value of y.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Introduction to Regression](http://dss.princeton.edu/online_help/analysis/regression_intro.htm)\n",
      "* [Good Regression Overview](http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf)\n",
      "* [Introduction to Multivariate Regression](http://www.apec.umn.edu/grad/jdiaz/IntroductiontoRegression.pdf)\n",
      "* [OSL in Matrix Form](http://www.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)\n",
      "* [Regression with Gradient Descent Function](https://github.com/KartikTalwar/OnlineCourses/blob/master/Stanford%20University/Machine%20Learning/02.%20Linear%20Regression%20with%20One%20Variable.md#cost-function)\n",
      "* [Multicollinearity and Singularity](http://dss.wikidot.com/multicollinearity-and-singularity)\n",
      "* [Multivariate Linear Regression Models](http://www.public.iastate.edu/~maitra/stat501/lectures/MultivariateRegression.pdf)\n",
      "* [Orthogonal Polynomials](https://www.cs.iastate.edu/~cs577/handouts/orthogonal-polys.pdf)\n",
      "* [Orthogonal Functions & Expansions](http://web.hep.uiuc.edu/home/serrede/P435/Lecture_Notes/P435_Supp_HO_01.pdf)\n",
      "* [The Analytic Theory of Matrix Orthogonal Polynomials](http://www.emis.de/journals/SAT/papers/11/11.pdf)\n",
      "* [Transfomations](http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch8.pdf)\n",
      "* [Regularization with Ridge penalties, the Lasso, and the Elastic Net for Regression with Optimal Scaling Transformations](https://openaccess.leidenuniv.nl/bitstream/handle/1887/12096/04.pdf?sequence=18)\n",
      "* [Cost Minimization Problem w/ Lagrangian](http://www.youtube.com/watch?v=PlZ0Mgu-9RY)\n",
      "* [Vector And Matrix Norms](http://www-solar.mcs.st-andrews.ac.uk/~clare/Lectures/num-analysis/Numan_chap1.pdf)\n",
      "* [Introduction to Multicolinearity](https://onlinecourses.science.psu.edu/stat501/node/79)\n",
      "* [Multicolinearity](http://www.chsbs.cmich.edu/fattah/courses/empirical/multicollinearity.html)\n",
      "* [Testing the assumptions of linear regression](http://people.duke.edu/~rnau/testing.htm)\n",
      "* [Multicolinearity](http://en.wikipedia.org/wiki/Multicollinearity)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}